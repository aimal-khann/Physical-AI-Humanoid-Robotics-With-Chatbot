<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-04-vla" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 4 - Vision-Language-Action (VLA) | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://aimal-khann.github.io/Physical-AI-Humanoid-Robotics-With-Chatbot/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://aimal-khann.github.io/Physical-AI-Humanoid-Robotics-With-Chatbot/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://aimal-khann.github.io/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/04-vla"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 4 - Vision-Language-Action (VLA) | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Welcome to Chapter 4: Vision-Language-Action (VLA). This chapter explores the exciting field of Vision-Language-Action (VLA), where robots leverage large language models (LLMs) and advanced perception to understand natural language commands and execute complex tasks in the physical world. This integration bridges the gap between human intent and robotic execution, leading to more intuitive and versatile robotic systems."><meta data-rh="true" property="og:description" content="Welcome to Chapter 4: Vision-Language-Action (VLA). This chapter explores the exciting field of Vision-Language-Action (VLA), where robots leverage large language models (LLMs) and advanced perception to understand natural language commands and execute complex tasks in the physical world. This integration bridges the gap between human intent and robotic execution, leading to more intuitive and versatile robotic systems."><link data-rh="true" rel="icon" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://aimal-khann.github.io/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/04-vla"><link data-rh="true" rel="alternate" href="https://aimal-khann.github.io/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/04-vla" hreflang="en"><link data-rh="true" rel="alternate" href="https://aimal-khann.github.io/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/04-vla" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"VLA","item":"https://aimal-khann.github.io/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/04-vla"}]}</script><link rel="stylesheet" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/assets/css/styles.732dc8a7.css">
<script src="/Physical-AI-Humanoid-Robotics-With-Chatbot/assets/js/runtime~main.f028fd29.js" defer="defer"></script>
<script src="/Physical-AI-Humanoid-Robotics-With-Chatbot/assets/js/main.f25c59c9.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/"><div class="navbar__logo"><img src="/Physical-AI-Humanoid-Robotics-With-Chatbot/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Physical-AI-Humanoid-Robotics-With-Chatbot/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/">Chapters</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/aimal-khann" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/01-ros2"><span title="ROS 2" class="linkLabel_WmDU">ROS 2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/02-gazebo-unity"><span title="Digital Twin" class="linkLabel_WmDU">Digital Twin</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/03-isaac"><span title="NVIDIA Isaac" class="linkLabel_WmDU">NVIDIA Isaac</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/04-vla"><span title="VLA" class="linkLabel_WmDU">VLA</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/05-capstone"><span title="Capstone" class="linkLabel_WmDU">Capstone</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/06-references"><span title="References" class="linkLabel_WmDU">References</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">VLA</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Chapter 4 - Vision-Language-Action (VLA)</h1></header><p>Welcome to Chapter 4: Vision-Language-Action (VLA). This chapter explores the exciting field of Vision-Language-Action (VLA), where robots leverage large language models (LLMs) and advanced perception to understand natural language commands and execute complex tasks in the physical world. This integration bridges the gap between human intent and robotic execution, leading to more intuitive and versatile robotic systems.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="41-introduction-to-vision-language-action-vla">4.1 Introduction to Vision-Language-Action (VLA)<a href="#41-introduction-to-vision-language-action-vla" class="hash-link" aria-label="Direct link to 4.1 Introduction to Vision-Language-Action (VLA)" title="Direct link to 4.1 Introduction to Vision-Language-Action (VLA)" translate="no">â€‹</a></h2>
<p>VLA systems enable robots to interpret human instructions given in natural language, understand the visual context of their environment, and then perform corresponding physical actions. This multidisciplinary field combines advancements in natural language processing (NLP), computer vision (CV), and robotics control.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="components-of-a-vla-system">Components of a VLA System<a href="#components-of-a-vla-system" class="hash-link" aria-label="Direct link to Components of a VLA System" title="Direct link to Components of a VLA System" translate="no">â€‹</a></h3>
<ul>
<li class=""><strong>Vision</strong>: Perceiving the environment using cameras, depth sensors, etc., and interpreting visual information.</li>
<li class=""><strong>Language</strong>: Understanding natural language commands, often processed by large language models (LLMs).</li>
<li class=""><strong>Action</strong>: Executing physical tasks through robot manipulators, mobile bases, or other effectors.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="42-voice-commands-with-whisper">4.2 Voice Commands with <code>Whisper</code><a href="#42-voice-commands-with-whisper" class="hash-link" aria-label="Direct link to 42-voice-commands-with-whisper" title="Direct link to 42-voice-commands-with-whisper" translate="no">â€‹</a></h2>
<p><code>Whisper</code> is an open-source neural network developed by OpenAI for robust speech-to-text transcription. It can convert spoken language into written text, providing a crucial input modality for VLA systems, allowing humans to interact with robots using voice.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="example-using-whisper-for-speech-to-text-python">Example: Using <code>Whisper</code> for Speech-to-Text (Python)<a href="#example-using-whisper-for-speech-to-text-python" class="hash-link" aria-label="Direct link to example-using-whisper-for-speech-to-text-python" title="Direct link to example-using-whisper-for-speech-to-text-python" translate="no">â€‹</a></h3>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># whisper_example.py</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> whisper</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Load the Tiny model for quick demonstration</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Other models: &#x27;base&#x27;, &#x27;small&#x27;, &#x27;medium&#x27;, &#x27;large&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> whisper</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">load_model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;tiny&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">transcribe_audio</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">audio_path</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    Transcribes an audio file using the Whisper model.</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    result </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">transcribe</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">audio_path</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> result</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&quot;text&quot;</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> __name__ </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;__main__&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># You would typically record audio from a microphone here.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># For demonstration, let&#x27;s assume we have an audio file named &quot;audio.mp3&quot;.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Placeholder for actual audio input</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Replace &quot;audio.mp3&quot; with a path to your actual audio file</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;Please provide an audio file path to transcribe:&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    audio_file </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token builtin">input</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">try</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        transcribed_text </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> transcribe_audio</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">audio_file</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string-interpolation string" style="color:#e3116c">f&quot;Transcribed text: </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">transcribed_text</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">except</span><span class="token plain"> Exception </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> e</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string-interpolation string" style="color:#e3116c">f&quot;Error during transcription: </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">e</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;Make sure you have an audio file at the specified path.&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<p><strong>Note</strong>: To run this example, you need to install the <code>whisper</code> package (<code>pip install -U openai-whisper</code>) and potentially <code>ffmpeg</code> for audio processing.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="43-llm-planning--ros-2-action-pipeline">4.3 LLM Planning â†’ ROS 2 Action Pipeline<a href="#43-llm-planning--ros-2-action-pipeline" class="hash-link" aria-label="Direct link to 4.3 LLM Planning â†’ ROS 2 Action Pipeline" title="Direct link to 4.3 LLM Planning â†’ ROS 2 Action Pipeline" translate="no">â€‹</a></h2>
<p>Large Language Models (LLMs) can be used to convert high-level natural language instructions into a sequence of executable robot actions. This involves prompting the LLM to generate a plan, which is then translated into ROS 2 actions or service calls for execution.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="conceptual-pipeline">Conceptual Pipeline<a href="#conceptual-pipeline" class="hash-link" aria-label="Direct link to Conceptual Pipeline" title="Direct link to Conceptual Pipeline" translate="no">â€‹</a></h3>
<ol>
<li class=""><strong>Voice Command</strong>: Human speaks a command (e.g., &quot;Robot, pick up the red block and place it on the table&quot;).</li>
<li class=""><strong>Speech-to-Text</strong>: <code>Whisper</code> converts voice to text.</li>
<li class=""><strong>LLM Planning</strong>: The text command is fed to an LLM, which generates a logical plan (e.g., &quot;detect red block&quot;, &quot;move to red block&quot;, &quot;grasp red block&quot;, &quot;move to table&quot;, &quot;release red block&quot;).</li>
<li class=""><strong>Task Decomposition &amp; ROS 2 Actions</strong>: The LLM&#x27;s plan is decomposed into a series of ROS 2 actions (e.g., calling a &quot;grasp&quot; action server, publishing to a &quot;move&quot; topic).</li>
<li class=""><strong>Execution</strong>: The robot executes the ROS 2 actions.</li>
<li class=""><strong>Feedback (Vision)</strong>: Visual perception continuously monitors the environment to confirm action success or detect failures, providing feedback to the LLM for replanning if needed.</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="example-llm-prompt-for-robotic-plan-conceptual">Example: LLM Prompt for Robotic Plan (Conceptual)<a href="#example-llm-prompt-for-robotic-plan-conceptual" class="hash-link" aria-label="Direct link to Example: LLM Prompt for Robotic Plan (Conceptual)" title="Direct link to Example: LLM Prompt for Robotic Plan (Conceptual)" translate="no">â€‹</a></h3>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">User Instruction: &quot;Move the green bottle to the kitchen counter.&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">LLM Prompt:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;Given a robot with capabilities:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- detect_object(object_name: str)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- pick_object(object_name: str)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- place_object(location_name: str)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- navigate_to_location(location_name: str)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Generate a step-by-step plan for the instruction &#x27;Move the green bottle to the kitchen counter&#x27;. Each step should be a call to one of the robot&#x27;s capabilities.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Plan:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1. detect_object(&#x27;green bottle&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. navigate_to_location(&#x27;green bottle&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. pick_object(&#x27;green bottle&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">4. navigate_to_location(&#x27;kitchen counter&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">5. place_object(&#x27;kitchen counter&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="exercises">Exercises<a href="#exercises" class="hash-link" aria-label="Direct link to Exercises" title="Direct link to Exercises" translate="no">â€‹</a></h2>
<ol>
<li class=""><strong>Whisper CLI</strong>: Experiment with the <code>Whisper</code> command-line interface to transcribe different audio inputs (e.g., your own voice, YouTube clips). Observe the accuracy for various accents and background noise levels.</li>
<li class=""><strong>Simple LLM Plan</strong>: Using a local or online LLM, create prompts that instruct the LLM to generate robot action plans for simple tasks like &quot;Stack the blue cube on the red cube&quot; given a predefined set of robot capabilities.</li>
<li class=""><strong>ROS 2 Action Client</strong>: Implement a ROS 2 action client in Python that sends a goal to a mock &quot;pick and place&quot; action server (you can simulate the server&#x27;s response for this exercise).</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/aimal-khann/Physical-AI-Humanoid-Robotics-With-Chatbot/tree/main/docs/04-vla.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/03-isaac"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">NVIDIA Isaac</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/05-capstone"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Capstone</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#41-introduction-to-vision-language-action-vla" class="table-of-contents__link toc-highlight">4.1 Introduction to Vision-Language-Action (VLA)</a><ul><li><a href="#components-of-a-vla-system" class="table-of-contents__link toc-highlight">Components of a VLA System</a></li></ul></li><li><a href="#42-voice-commands-with-whisper" class="table-of-contents__link toc-highlight">4.2 Voice Commands with <code>Whisper</code></a><ul><li><a href="#example-using-whisper-for-speech-to-text-python" class="table-of-contents__link toc-highlight">Example: Using <code>Whisper</code> for Speech-to-Text (Python)</a></li></ul></li><li><a href="#43-llm-planning--ros-2-action-pipeline" class="table-of-contents__link toc-highlight">4.3 LLM Planning â†’ ROS 2 Action Pipeline</a><ul><li><a href="#conceptual-pipeline" class="table-of-contents__link toc-highlight">Conceptual Pipeline</a></li><li><a href="#example-llm-prompt-for-robotic-plan-conceptual" class="table-of-contents__link toc-highlight">Example: LLM Prompt for Robotic Plan (Conceptual)</a></li></ul></li><li><a href="#exercises" class="table-of-contents__link toc-highlight">Exercises</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-With-Chatbot/docs/">Chapters</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">GitHub</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/aimal-khann" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2025 Physical AI & Humanoid Robotics.</div></div></div></footer><div class="chat-widget"><button class="chat-button">ðŸ’¬</button></div></div>
</body>
</html>